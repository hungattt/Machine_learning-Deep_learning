{"cells":[{"cell_type":"markdown","metadata":{"id":"IZG7hNuyuPCJ"},"source":["# Thực hiện xây dựng mạng Neural Network\n","\n","Trong tuần này, bạn sẽ thực hiện xây dựng Neural Network đơn giản với:\n","- 2 lớp ẩn với hàm phi tuyến Relu\n","- một lớp phân loại softmax\n","\n","Bạn có thể mở rộng notebook này để có mạng nhiều lớp hơn."]},{"cell_type":"markdown","metadata":{"id":"tJk5UrQ6nXJK"},"source":["Chú ý: Bài tập có một số thiếu sót và cách khắc phục ở dưới đây. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Boozh29bidzo"},"source":["**Fix bug**: \n","\n","Xem chi tiết trong TODO 8\n","\n","- TODO 8: \n","\n","```Python\n","db3 = 1/m * np.sum(E3, keepdims=True, axis=1)\n","```\n","\n","- TODO 8: \n","\n","```Python\n","m = X_train.shape[0]\n","``` "]},{"cell_type":"markdown","metadata":{"id":"lZTfsvxWssun"},"source":["## 1 - Khai báo một số hàm bổ sung"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1682078684905,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"MEseTbtQplF8"},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XpT1677QuPCV"},"source":["## 2 - Quá trình thực hiện \n","\n","Quá trình toàn bộ mô hình sẽ theo các bước sau\n","\n","- Khởi tạo các tham số W, b.\n","- Triển khai `Forward Propagation Module` (lan truyền thuận). Như đường màu tím trong hình bên dưới.\n","- Tính giá trị mất mát cost.\n","- Thực hiện `Backward Propagation Module` (lan truyền ngược). Như đường màu xanh trong hình bên dưới. \n","- Cập nhật các tham số. \n","\n","\n","<caption><center> Ảnh 1</center></caption><br>\n","\n","**Lưu ý**: với mỗi forward function, sẽ có backward function tương ứng. Đó là lý do mà ở mỗi forward module chúng ta sẽ lưu lại các giá trị này trong `cache` để thuận tiện cho việc tính `gradients`. \n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"KKhUIXrkuPCW"},"source":["## 3 - Khởi tạo\n"]},{"cell_type":"markdown","metadata":{"id":"7rimzl3wumQo"},"source":["**Hướng dẫn**:\n","- Khởi tạo giá trị random cho ma trận. Sử dụng `np.random.randn(shape)*0.01`.\n","- Khởi tạo `0` cho bias. Sử dụng `np.zeros(shape)`."]},{"cell_type":"markdown","metadata":{"id":"Y_x3JL9zkGYu"},"source":["Các bộ tham số giữa các lớp sẽ có chiều như sau:\n","\n","- Lớp 1:\n","  - $\\textbf{W}^{(1)} \\in \\mathbf{R}^{d^{(1)} \\times  n } $ \n","  - $b^{(1)} \\in \\mathbf{R}^{ d^{(1)} \\times 1 }$\n","- Lớp 2:\n","  - $\\textbf{W}^{(2)} \\in \\mathbf{R}^{d^{(2)} \\times d^{(1)}} $ \n","  - $b^{(2)} \\in \\mathbf{R}^{ d^{(2)} \\times 1 }$\n","- Lớp 3:\n","  - $\\textbf{W}^{(3)} \\in \\mathbf{R}^{c \\times d^{(2)}} $ \n","  - $b^{(3)} \\in \\mathbf{R}^{ c \\times 1 }$\n","\n","Biến `num_classes` đại diện cho số nhãn $c$ trong trường hợp này."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1682078231679,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"fSoEHkUJuPCX"},"outputs":[],"source":["def initialize_parameters(n, d_1, d_2, num_classes):\n","    \"\"\"\n","    Hàm khởi tạo tham số ban đầu\n","    Đầu vào:\n","      n: int\n","        Chiều của x đầu vào\n","      d_1: int\n","        Chiều của lớp ẩn 1\n","      d_2: \n","        Chiều của lớp ẩn 2\n","      num_classes:\n","        Số lượng nhãn\n","    Đầu ra:\n","      parameters: python dictionary\n","      Bao gồm:\n","        W1: ma trận W1 lớp 1 với chiều (d_1, n)\n","        b1: vector bias b1 lớp 1 với chiều (d_1, 1)\n","        W2: ma trận W2 lớp 2 với chiều (d_2, d_1)\n","        b2: vector bias b2 lớp 2 với chiều (d_2, 1)\n","        W3:  ma trận W3 lớp 3 với chiều (num_classes, d_2)\n","        b3: vector bias b3 lớp 3 với chiều (num_classes, 1)\n","    \"\"\"\n","    \n","    # Khởi tạo tham số\n","\n","    np.random.seed(42)\n","    W1 = np.random.randn(d_1, n) * 0.01\n","    b1 = np.zeros((d_1, 1))\n","    W2 = np.random.randn(d_2, d_1) * 0.01\n","    b2 = np.zeros((d_2, 1))\n","    W3 = np.random.randn(num_classes, d_2) * 0.01\n","    b3 = np.zeros((num_classes, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2,\n","                  \"W3\": W3,\n","                  \"b3\": b3,\n","                  }\n","    \n","    return parameters    "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682078231679,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"1H9ylZ8vuPCc","outputId":"d4db4afa-4012-4d10-eb76-4353f24e1cd7","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["W1.shape = (256, 784)\n","b1.shape = (256, 1)\n","W2.shape = (128, 256)\n","b2.shape = (128, 1)\n","W3.shape = (10, 128)\n","b3.shape = (10, 1)\n"]}],"source":["parameters = initialize_parameters(784, 256, 128, 10)\n","print(\"W1.shape = {}\".format(parameters[\"W1\"].shape))\n","print(\"b1.shape = {}\".format(parameters[\"b1\"].shape))\n","print(\"W2.shape = {}\".format(parameters[\"W2\"].shape))\n","print(\"b2.shape = {}\".format(parameters[\"b2\"].shape))\n","print(\"W3.shape = {}\".format(parameters[\"W3\"].shape))\n","print(\"b3.shape = {}\".format(parameters[\"b3\"].shape))"]},{"cell_type":"markdown","metadata":{"id":"BA0wIS9ouPCs"},"source":["## 4 - Lan truyền thuận (Forward propagation module)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xJ4OlH6Ru3uB"},"source":["### 4.1 - Lan truyền thuận tuyến tính (Linear Forward)\n","\n","Sau khi khởi tạo các tham số cần thiết, bây giờ bạn sẽ thực hiện forward propagation module. Bạn sẽ thực hiện theo thứ tự sau:\n","\n","- Tuyến tính (Linear) \n","- Tuyến tính $\\rightarrow $ Phi tuyến (Linear $\\rightarrow $ Activation)\n","\n","\n","#### 4.1.1. Trên 1 điểm dữ liệu\n","\n","\n","Công thức lan truyền thuận tuyến tính như sau:\n","\n","$$\\textbf{z}^{(l)} = \\textbf{W}^{(l)}\\textbf{a}^{(l-1)} +\\textbf{b}^{(l)}\\tag{4}$$\n","\n","trong đó $\\textbf{a}^{(0)} = \\textbf{x}$. \n","\n","\n","#### 4.1.2. Trên toàn tập dữ liệu\n","\n","\n","Công thức Linear Forward như sau:\n","\n","\n","\n","$$\\textbf{Z}^{(l)} = \\textbf{W}^{(l)}\\textbf{A}^{(l-1)} +\\textbf{b}^{(l)}\\tag{5}$$\n","\n","\n","trong đó $\\textbf{A}^{(0)} = \\textbf{X}$ \n","\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1682078989958,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"iB3SvCIhuPCt"},"outputs":[],"source":["def linear_forward(A_pre, W, b):\n","    \"\"\"\n","    Tiến hành lan truyền thuận tuyến tính (linear forward)\n","    Đầu vào:\n","      A_pre: \n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    Đầu ra:\n","      Z: \n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: python tuple\n","        Miêu tả: Bao gồm các biến A_pre, W và b\n","    \"\"\"\n","    \n","    Z = np.dot(W, A_pre) + b \n","    \n","    cache = (A_pre, W, b)\n","    \n","    return Z, cache"]},{"cell_type":"markdown","metadata":{"id":"l4CtxMAHUGw0"},"source":["Test code"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1682078232242,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"mPxh6ht6uPCx","outputId":"14e59fe9-42de-465d-beba-386df6d6fa31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Z.shape: (128, 60000)\n"]}],"source":["try:\n","  A1 = np.ones((256, 60000))\n","  W2 = np.random.normal(size=((128, 256)))\n","  b2 = np.ones(((128, 1)))\n","\n","  Z, linear_cache = linear_forward(A1, W2, b2)\n","  print(\"Z.shape: {}\".format(Z.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: {}\", e)"]},{"cell_type":"markdown","metadata":{"id":"LXNBmND6fV-z"},"source":["### 4.2. Lan truyền thuận phi tuyến (Activation Forward)"]},{"cell_type":"markdown","metadata":{"id":"whvJKWdLfoXA"},"source":["#### 4.2.1 Công thức RELU\n","\n","$$f(z) = \\left\\{\\begin{matrix}\n","0 \\ \\forall z \\leq 0 \\\\\n","z \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right. = \\max\\left \\{0, z  \\right \\} = z\\textbf{1}_{z>1}$$"]},{"cell_type":"markdown","metadata":{"id":"TyPiNkq7gNhv"},"source":["**Chú ý:** Các phép tính trong bài toán này đều sử dụng ma trận cho thuật toán Gradient Descent thay vì vector trong bài tập Softmax"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682078232242,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"G7Csx0CHoeZn"},"outputs":[],"source":["def relu(Z):\n","    \"\"\"\n","    Thực hiện hàm RELU trên ma trận Z\n","    Đầu vào:\n","      Z: \n","        Dạng: numpy array \n","        Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    Đầu ra:\n","      A:\n","        Dạng: numpy array \n","        Miêu tả: Giá trị A sau khi đưa Z qua Relu\n","        Chiều: (Chiều của lớp hiện tại, 60000)\n","        Ví dụ: (256, 60000)\n","      cache:\n","        Dạng: numpy array \n","        Miêu tả: \n","          Ma trận Z. Đầu ra của phép nhân tuyến tính (hay đầu vào của phép phi tuyến)\n","          Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","        Ví dụ: (256, 60000)\n","    \"\"\"\n","    \n","    # Lập trình tại đây\n","\n","    A = np.maximum(0,Z)\n","    \n","    cache = Z \n","\n","    return A, cache"]},{"cell_type":"markdown","metadata":{"id":"_XONCPgw-Qsf"},"source":["#### 4.2.2. Công thức Softmax\n"]},{"cell_type":"markdown","metadata":{"id":"qK1_psFQgFr8"},"source":["\n","Trong trường hợp này ta sẽ lấy softmax trên từng cột của $\\textbf{Z}$ với $\\textbf{Z} \\in \\mathbf{R}^{c \\times m} $\n","\n","Với:\n","\n","- $c$: Số nhãn\n","- $m$: Số lượng điễm dữ liệu\n","\n","\n","$$\\hat{\\textbf{Y}} = \\text{softmax}(\\textbf{Z}) =  \\begin{bmatrix}\n","\\text{softmax}(\\begin{bmatrix}\n","z_1^1 \\\\\n","\\ \\vdots \\\\ \n","z_c^1 \\\\ \n","\\end{bmatrix}) & \\text{softmax}(\\begin{bmatrix}\n","z_1^2 \\\\\n","\\ \\vdots \\\\ \n","z_c^2 \\\\ \n","\\end{bmatrix}) & \\dots & \\text{softmax}(\\begin{bmatrix}\n","z_1^m \\\\\n","\\ \\vdots \\\\ \n","z_c^m \\\\ \n","\\end{bmatrix})\n","\\end{bmatrix} $$"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":542,"status":"ok","timestamp":1682079622389,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"_wmZVN138GKW"},"outputs":[],"source":["def softmax(Z):\n","  \"\"\"\n","  Thực hiện hàm softmax trên ma trận Z\n","  Đầu vào:\n","    Z: \n","      Dạng: numpy array \n","      Miêu tả: Ma trận Z. Đầu ra của phép nhân tuyến tính\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  Đầu ra:\n","    Y_hat:\n","      Dạng: numpy array \n","      Miêu tả: Chuẩn hóa ma trận Z theo phân phối softmax có tổng các giá trị bằng 1\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","    cache:\n","      Dạng: numpy array \n","      Miêu tả: \n","        Ma trận Z. Đầu ra của phép nhân tuyến tính\n","        Việc lưu trữ này để sử dụng cho thuật toán lan truyền ngược\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","      Ví dụ: (10, 60000)\n","  \"\"\"\n","\n","  e_Z = np.exp(Z)\n","\n","  Y_hat = e_Z/ e_Z.sum(axis=0)\n","\n","  print(e_Z.shape, e_Z.sum(axis=0).shape)\n","\n","  cache = Z\n","  \n","  return Y_hat, cache"]},{"cell_type":"markdown","metadata":{"id":"j3rPCVP3jGw1"},"source":["Test code\n"]},{"cell_type":"markdown","metadata":{"id":"mNcZEbYY-c3k"},"source":["Kiểm tra lại các cột sau khi sử dụng Softmax có cho kết quả tổng bằng 1 hay không."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682079622872,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vQm9e4cv8ItL","outputId":"eab180bc-4e3c-4480-8aa9-8fd7d7788731"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 60000) (60000,)\n"]},{"data":{"text/plain":["1.0"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["Z = np.ones(((10, 60000)))\n","Y_hat, _ = softmax(Z)\n","\n","np.sum(Y_hat[:,3])"]},{"cell_type":"markdown","metadata":{"id":"EJMHwVRIuPC3"},"source":["### 4.3 - Lan truyền thuận trên một lớp (Linear-Activation Forward)\n","\n","Trong notebook này, bạn sẽ sử dụng activation function `RELU`:\n","\n","<!-- - **Sigmoid**: $ \\text{Sigmoid}(\\textbf{Z}) = \\sigma(\\textbf{Z}) = \\sigma(\\textbf{W} \\textbf{A} + \\textbf{b}) = \\frac{1}{ 1 + e^{-(\\textbf{W} \\textbf{A} + \\textbf{b})}}$. Function này sẽ trả về 2 giá trị \"`A`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau: \n","```python\n","A, activation_cache = sigmoid(Z)\n","``` -->\n","\n","- **ReLU**: công thức toán của ReLU là $\\textbf{A} = \\text{Relu}(\\textbf{Z}) = \\text{max}(0, \\textbf{Z})$.  Function này sẽ trả về 2 giá trị \"`a`\" (giá trị activation) và \"`cache`\" (chứa giá trị của `Z`) - Các giá trị này sẽ được sử dụng cho Backward Function. Để sử dụng, bạn chỉ cần gọi theo mẫu sau: \n","``` python\n","A, activation_cache = relu(Z)\n","```"]},{"cell_type":"markdown","metadata":{"id":"sexU0YLZSmWm"},"source":["- Lớp 1:\n","\n","$$\\textbf{A}^{(1)} = \\text{Relu}(\\textbf{Z}^{(1)}) $$\n","\n","- Lớp 2:\n","\n","$$\\textbf{A}^{(2)} = \\text{Relu}(\\textbf{Z}^{(2)}) $$\n","\n","- Lớp 3 (Lớp cuối):\n","\n","$$\\textbf{A}^{(3)} = \\hat{\\textbf{Y}} = \\text{Softmax}(\\textbf{Z}^{(3)}) \\tag{6}$$"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682078232243,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"4WefOTnWuPC4"},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Tiến hành non-linear forward\n","    A_prev, W và B đưa qua hàm linear_forward thu được Z\n","    Từ đó Z sẽ được đưa qua hàm phi tuyến RELU\n","    Đầu vào:\n","      A_prev:\n","        Dạng: numpy array\n","          Miêu tả: Đầu ra của lớp phía trước trên toàn bộ tập dữ liệu\n","          Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      W:\n","        Dạng: numpy array\n","        Miêu tả: Ma trận tham số của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      b:\n","        Dạng: numpy array\n","        Miêu tả: Vector bias của lớp hiện tại\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","      activation:\n","        Dạng: python string\n","        Miêu tả: Tên hàm activation. Ví dụ \"softmax\" or \"relu\"\n","\n","    Returns:\n","      A: \n","        Dạng: numpy array\n","        Miêu tả: Đầu ra của phép phi tuyến hay đầu ra của hàm activation function\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache: \n","        Dạng: python tuple: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache: tuple (A_pre, W, b) đầu vào của phép tuyến tính được lưu lại \n","          activation_cache: Z đầu vào của phép phi tuyến được lưu lại \n","    \"\"\"\n","    # Ta sẽ phân loại việc biến đổi tuyến tính tùy theo đầu vào activation\n","    if activation == \"relu\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    elif activation == \"softmax\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","  \n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"markdown","metadata":{"id":"IabUnrabXmop"},"source":["Test code"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682078232243,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"0jWyGRr_uPC7","outputId":"68eb7574-8c8e-4489-f863-271f3aa2c725"},"outputs":[{"name":"stdout","output_type":"stream","text":["A.shape: (10, 60000)\n","activation_cache.shape hay Z.shape: (10, 60000)\n","linear_cache bao gồm A2 với chiều A2.shape: (256, 60000), W3 với chiều W3.shape: (10, 256), b3 với chiều b3.shape: (10, 1)\n"]}],"source":["try:\n","  A2 = np.ones((256, 60000))\n","  W3 = np.random.normal(size=((10, 256)))\n","  b3 = np.ones(((10, 1)))\n","\n","  A, cache = linear_activation_forward(A2, W3, b3, activation='softmax')\n","  print(\"A.shape: {}\".format(A.shape))\n","\n","  # cache là một tuple\n","\n","  linear_cache, activation_cache = cache\n","  A2, W3, b3 = linear_cache\n","  \n","  print(\"activation_cache.shape hay Z.shape: {}\".format(activation_cache.shape))\n","  print(\"linear_cache bao gồm A2 với chiều A2.shape: {}, W3 với chiều W3.shape: {}, b3 với chiều b3.shape: {}\".format(A2.shape, W3.shape, b3.shape))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"PwsazF-kuPDI"},"source":["## 5 - Hàm mất mát (Cost function)\n","\n","Công thức toán của cross-entropy cost $J$: $$J(\\textbf{W}) = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\sum\\limits_{j = 1}^{c} \\textbf{y}^{(i)}\\log (\\hat{\\textbf{y}}^{(i)})\\tag{7}$$\n","\n","Với\n","- $c$: số nhãn\n","- $m$: số điểm dữ liệu\n","\n","Chú ý trong bài này đầu vào của hàm là theo toàn bộ tập dữ liệu.\n"]},{"cell_type":"markdown","metadata":{"id":"KT_l4MKbNC9v"},"source":["**TODO 1**: Lập trình công thức hàm Cost Function"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":516,"status":"ok","timestamp":1682080159303,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"_qYYnRcouPDI"},"outputs":[],"source":["def compute_cost(Y_hat, Y):\n","    \"\"\"\n","    Tiến hành categorical cross-entropy trên toàn bộ tập dữ liệu\n","    Đầu vào:\n","    Y_hat:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của lớp softmax trên toàn bộ tập dữ liệu\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    Y:\n","      Dạng: numpy array\n","      Miêu tả: Nhãn của dữ liệu (Dạng One hot)\n","      Chiều: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","\n","    Returns:\n","      Dạng: numpy float\n","      Miêu tả: Giá trị mất mát trên toàn bộ tập dữ liệu\n","    \"\"\"\n","    \n","    # Lập trình tại đây\n","\n","    m = Y.shape[1]\n","\n","    # Nhân Hadamard và lấy tổng theo cột.\n","    # Lấy tổng theo cột vì chiều đầu vào có số dòng là số lượng nhãn cho nên ta\n","    # cộng tất cả giá trị trong cùng 1 cột lại với nhau để tìm giá trị mất mát\n","    # trên 1 điểm dữ liệu đó\n","\n","    cost = np.sum((Y * np.log(Y_hat)), axis=0)\n","    \n","\n","    # Lấy tổng theo theo cột kèm theo chia trung bình\n","    cost = - 1/m * np.sum(cost)\n","    \n","    return cost"]},{"cell_type":"markdown","metadata":{"id":"7bhcwVHWYlgW"},"source":["Test code"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1682080373856,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"nbqLdQsjuPDK","outputId":"ab0255e6-0d7c-4be1-cd64-a4557f1b2434"},"outputs":[{"name":"stdout","output_type":"stream","text":["cost = 2.3025850929940455\n"]}],"source":["Y =  np.array(\n","      [[0., 0.],\n","       [0., 0.],\n","       [0., 1.],\n","       [0., 0.],\n","       [0., 0.],\n","       [1., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]])\n","\n","Y_hat =  np.array(\n","      [[0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.1, 0.05],\n","       [0.2, 0.2],\n","       [0.05, 0.05],\n","       [0.2, 0.1],\n","       [0.1, 0.1],\n","       [0.05, 0.2],\n","       [0.05, 0.1],\n","       [0.05, 0.1]])\n","\n","print(\"cost = {}\".format(compute_cost(Y_hat, Y)))"]},{"cell_type":"markdown","metadata":{"id":"NmUY02AzNXqB"},"source":["**Kết quả mong đợi**: \n","\n","```\n","cost = 2.3025850929940455\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"iqtGNVroYm0h"},"source":["Giá trị này tương đương với $-(\\text{log}(0.2) + \\text{log}(0.05))$"]},{"cell_type":"markdown","metadata":{"id":"7f-Ms322uPDN"},"source":["## 6 - Lan truyền ngược (Backward propagation module)\n","\n","Cũng giống như lan truyền thuận (forward propagation), bây giờ bạn sẽ thực hiện các function cho backpropagation. Chú ý lan truyền thuận được sử dụng để tính toán gradient của hàm mất mát với các tham số (parameters).\n"]},{"cell_type":"markdown","metadata":{"id":"VAh6SysJZJ1Z"},"source":["Mỗi một hàm lan truyền thuận phía trên sẽ đi kèm một hàm lan truyền ngược"]},{"cell_type":"markdown","metadata":{"id":"axA8WuxqZizJ"},"source":["### 6.1 - Lan truyền ngược tuyến tính (Linear backward)"]},{"cell_type":"markdown","metadata":{"id":"lj8N7e2suPDN"},"source":["\n","Tại lớp thứ $l$, ta có: $\\textbf{Z}^{(l)} = \\textbf{W}^{(l)} \\textbf{A}^{(l-1)} + \\textbf{b}^{(l)}$ (Activation tương ứng).\n","\n","Nhiệm vụ tiên quyết là tính được các giá trị $\\textbf{E}^{(l)}$ vì ta có thể dùng $\\textbf{E}^{(l)}$ để tính ra được $\\textbf{E}^{(l-1)}$ \n","\n","Giá sử bạn đã tính đạo hàm của $\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}}$. \n","\n","\n","\n","Bạn muốn tính $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)})$.\n","\n","3 giá trị $(d\\textbf{W}^{(l)}, d\\textbf{b}^{(l)}, d\\textbf{A}^{(l-1)}$ được tính toán từ $d\\textbf{Z}^{(l)}$ thông qua các công thức sau:\n","\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{W}^{(l)}$\n","\n","$$\n","d\\textbf{W}^{(l)} = \\frac{\\partial {J} }{\\partial \\textbf{W}^{(l)}} = \\frac{1}{m} d\\textbf{Z}^{(l)} \\textbf{A}^{(l-1) T} = \\frac{1}{m} \\textbf{E}^{(l)} \\textbf{A}^{(l-1) T} \\tag{8}\n","$$\n","\n","- Đạo hàm của hàm mất mát trên $\\textbf{b}^{(l)}$.Một cách cụ thể thì đây chính là trung bình tổng các cột của ma trận ${E}^{(l)}$\n","\n","\n","$$ d\\textbf{b}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{b}^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} d\\textbf{Z}^{(l)(i)} = \\frac{1}{m} \\sum_{i = 1}^{m} \\textbf{E}^{(l)(i)} \\tag{9}$$\n","\n","      \n","- Đạo hàm của hàm mất mát trên $\\textbf{A}^{(l-1)}$\n","\n","$$ d\\textbf{A}^{(l-1)} = \\frac{\\partial J }{\\partial \\textbf{A}^{(l-1)}} = \\textbf{W}^{(l) T} d\\textbf{Z}^{(l)} = \\textbf{W}^{(l) T} \\textbf{E}^{(l)} \\tag{10}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QaXb8X5maclW"},"source":["**TODO 2:** Lập trình các công thức trên cho hàm lan truyền ngược tuyến tính"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682078232243,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"9Qp5rYoquPDO"},"outputs":[],"source":["def linear_backward(E, cache):\n","    \"\"\"\n","    Tính toán đạo hàm của hàm mất mát trên A, W và b\n","    Đầu vào:\n","      E hoặc dZ:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra của phép tuyến tính tại lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache \n","        Dạng: Python tuple\n","        Miêu tả: Biến lưu trữ (cache) của lớp linear\n","        Bao gồm các biến A_prev, W và b\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây    \n","\n","    A_prev, W, b = cache\n","    \n","    m = A_prev.shape[1]\n","\n","    # Tính gradient trên W \n","    dW = 1/m * np.dot(E, A_prev.T)\n","    \n","    # Tính gradient trên b\n","    db = 1/m * np.sum(E, keepdims=True, axis=1)\n","\n","    # Tính gradient trên A phía trước\n","    dA_prev = np.dot(W.T, E)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":878,"status":"ok","timestamp":1682080608133,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"eWjslJ75uPDS","outputId":"0e9ae827-ab6d-4b40-ff73-4395417ab96e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chiều của dA1: (256, 60000)\n","Chiều của dW2: (128, 256)\n","Chiều của db2: (128, 1)\n","Giá trị dA1 được tính đúng?: True\n","Giá trị dW2 được tính đúng?: True\n","Giá trị db2 được tính đúng?: True\n"]}],"source":["try:\n","  # Test trên layer 2\n","  E2 = dZ = np.ones((128, 60000))\n","  A1 = np.ones((256, 60000))\n","  W2 = np.ones((128, 256))\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","\n","  dA1, dW2, db2 = linear_backward(E2, linear_cache)\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW2: {}'.format(dW2.shape))\n","  print('Chiều của db2: {}'.format(db2.shape))\n","  print('Giá trị dA1 được tính đúng?: {}'.format(np.array_equal(np.full(A1.shape, fill_value=128.), dA1)))\n","  print('Giá trị dW2 được tính đúng?: {}'.format(np.array_equal(np.full(W2.shape, fill_value=1.), dW2)))\n","  print('Giá trị db2 được tính đúng?: {}'.format(np.array_equal(np.full(b2.shape, fill_value=1.), db2)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"JBV-QJffuPDU"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW2: (128, 256)\n","Chiều của db2: (128, 1)\n","Giá trị dA1 được tính đúng?: True\n","Giá trị dW2 được tính đúng?: True\n","Giá trị db2 được tính đúng?: True\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"_Jpgrx_OfUhY"},"source":["### 6.2 - Lan truyền ngược phi tuyến (Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"WuK2em4AfWe6"},"source":["**TODO 3**: Lập trình đạo hàm của RELU"]},{"cell_type":"markdown","metadata":{"id":"QEbW0Qh0uPDU"},"source":["\n","\n","\n","Nếu $f(.)$  là activation function `relu_backward` được tính: $$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$\n","\n","- Với **RELU**\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z < 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","DNE \\ \\ \\text{if} \\ \\ z = 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","**RELU** `không có đạo` hàm tại 0 tuy nhiên khi lập trình ta sẽ cài đặt sẵn với trường hơp $z=0$ đạo hàm của RELU sẽ bằng `0`. Vậy công thức trở thành. \n","\n","$$f'(z) = \\left\\{\\begin{matrix}\n","0 \\ \\ \\forall z <= 0 \\\\\n","1 \\ \\ \\forall z > 0 \\\\\n","\\end{matrix}\\right.$$\n","\n","Cùng lập trình đạo hàm này với trình tự:\n","\n","- Tất cả những giá trị của Z nhỏ hơn hoặc bằng 0 sẽ được đặt thành 0\n","- Tất cả những giá trị của Z lớn hơn 0 sẽ được đặt thành 1"]},{"cell_type":"markdown","metadata":{"id":"0CR7gSLyDE7G"},"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078232804,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"c4mvUgNJjOzk"},"outputs":[],"source":["def relu_derivative(Z):\n","  \"\"\"\n","  Thực hiện đạo hàm của RELU trên ma trận Z\n","  Đầu vào:\n","    Z:\n","      Dạng: numpy array\n","      Miêu tả: Đầu ra của phép nhân tuyến tính và cũng là đầu vào của hàm activation\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  Đầu ra:\n","    Z_grad:\n","      Dạng: numpy array\n","      Miêu tả: Đạo hàm của hàm RELU với Z\n","      Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","  \"\"\"\n","  # Lập trình tại đây\n","    \n","  Z_grad = np.array(Z, copy=True) \n","  \n","  Z_grad[Z <= 0] = 0\n","  Z_grad[Z > 0] = 1\n","\n","  return Z_grad \n"]},{"cell_type":"markdown","metadata":{"id":"hdPfw7FPksJc"},"source":["Test code"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682080819003,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"xNb9JCDSktI2","outputId":"91cfe550-7083-4179-8b78-363eb8f424c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chiều của đạo hàm hàm RELU trên Z1: (128, 60000)\n","Chiều của đạo hàm hàm RELU trên Z2: (128, 60000)\n","Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: True\n","Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: True\n"]}],"source":["try:\n","  # Test trên layer 2\n","  Z1 = np.full((128, 60000), 2.)\n","  Z2 = np.full((128, 60000), -2.)\n","\n","  Z1_derivative = relu_derivative(Z1)\n","  Z2_derivative = relu_derivative(Z2)\n","\n","\n","  print('Chiều của đạo hàm hàm RELU trên Z1: {}'.format(Z1_derivative.shape))\n","  print('Chiều của đạo hàm hàm RELU trên Z2: {}'.format(Z2_derivative.shape))\n","  print('Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: {}'.format(np.array_equal(Z1_derivative, np.ones(Z1.shape))))\n","  print('Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: {}'.format(np.array_equal(Z2_derivative, np.zeros(Z2.shape))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"S4uwd6jtmtK5"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Chiều của đạo hàm hàm RELU trên Z1: (128, 60000)\n","Chiều của đạo hàm hàm RELU trên Z2: (128, 60000)\n","Giá trị đạo hàm hàm RELU trên Z1 đã được tính đúng?: True\n","Giá trị đạo hàm hàm RELU trên Z2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"_zNpCYprm3K2"},"source":["**TODO 4**: Lập trình hàm lan truyền ngược trên hàm phi tuyến\n","\n","Cụ thể là đạo hàm của **hàm mất mát** trên giá trị đầu vào hàm phi tuyến hay nói cách khác chính là:\n","\n","$$\\textbf{E}^{(l)} = d\\textbf{Z}^{(l)} = \\frac{\\partial J }{\\partial \\textbf{Z}^{(l)}} = d\\textbf{A}^{(l)} * f'(\\textbf{Z}^{(l)})$$"]},{"cell_type":"markdown","metadata":{"id":"KhPqOTFOnZPk"},"source":["Cho nên đầu vào của hàm tính giá trị $\\textbf{E}^{(l)}$ sẽ bao gồm 2 giá trị:\n","- Đạo hàm của hàm mất mát với đầu ra phép phi tuyến ở lớp hiện tại đã được tính trước: $d\\textbf{A}^{(l)}$\n","- Giá trị $\\textbf{Z}$ đã lưu lại khi thực hiện phi tuyến theo chiều thuận\n","\n","Hàm này sẽ thực hiện nhiệm vụ:\n","\n","- Sử dụng hàm `relu_derivative` để tính đạo hàm $f'(\\textbf{Z}^{(l)})$ của hàm RELU với $\\textbf{Z}$\n","- Sau đó lấy đầu vào $d\\textbf{A}^{(l)}$ nhân Hadamard với giá trị tìm được "]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078233387,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"dgi9ZEUDpz9E"},"outputs":[],"source":["def relu_backward(dA, cache):\n","    \"\"\"\n","    Thực hiện lan truyền ngược qua hàm phi tuyến RELU\n","    Hay nói cách khác đạo hàm của hàm mất mát trên Z\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: numpy array\n","        Miêu tả: Z được lưu lại từ hàm linear-activation forward\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    Đầu ra:\n","      E (dZ):\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của giá trị mất mát với Z\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    Z = cache\n","\n","    # Sử dụng hàm relu_derivative\n","    Z_derivative = relu_derivative(Z)\n","\n","    # Áp dụng công thức để tính E\n","    E = dZ = dA * Z_derivative\n","    \n","    return E"]},{"cell_type":"markdown","metadata":{"id":"mLAd4St6TPYY"},"source":["Test Code"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078233387,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vu4-QHRrTRCw","outputId":"cb3ebec3-eb11-4966-cd95-3f80f9c2ea22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chiều của E1: (128, 60000)\n","Chiều của E2: (128, 60000)\n","Giá trị E1 đã được tính đúng?: True\n","Giá trị E2 đã được tính đúng?: True\n"]}],"source":["try:\n","  dA1 = np.full((128, 60000), -2.)\n","  cache1 = np.full((128, 60000), -3.)\n","  E1 = relu_backward(dA1, cache1)\n","\n","  dA2 = np.full((128, 60000), 1.)\n","  cache2 = np.full((128, 60000), 1.)\n","  E2 = relu_backward(dA2, cache2)\n","\n","  print('Chiều của E1: {}'.format(E1.shape))\n","  print('Chiều của E2: {}'.format(E2.shape))\n","  print('Giá trị E1 đã được tính đúng?: {}'.format(np.array_equal(E1, np.full(E1.shape, 0.))))\n","  print('Giá trị E2 đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"BxeqogguVgv-"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Chiều của E1: (128, 60000)\n","Chiều của E2: (128, 60000)\n","Giá trị E1 đã được tính đúng?: True\n","Giá trị E2 đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"bcKXV9JwVrwD"},"source":["### 6.3 Lan truyền trên một lớp (Linear-Activation backward)"]},{"cell_type":"markdown","metadata":{"id":"qq68t8InVzYM"},"source":["**TODO 5:** Kết hợp phần `6.1` và `6.2` để tạo thành một hàm đầy đủ việc lan truyền ngược trên một lớp ẩn bất kỳ. Chú ý **đường màu xanh**."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682078233956,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"Rc4-K95KuPDV"},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Thực hiện lan truyền ngược trên lớp này\n","    Sử dụng kết hợp relu_backward và hàm linear_backward\n","    Đầu vào:\n","      dA:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l\n","        Chiều: (Chiều của lớp hiện tại, Số lượng điểm dữ liệu)\n","      cache:\n","        Dạng: Python tuple\n","        Miêu tả: (linear_cache, activation_cache)\n","        Bao gồm:\n","          linear_cache: \n","            Dạng: Python tuple\n","            Miêu tả: cache lưu lại từ phép tuyến tính thuận: (A_pre, W, b)\n","          activation_cache:\n","            Dạng: Numpy array\n","            Miêu tả: cache lưu lại từ phép phi tính thuận: (Z)\n","      activation:\n","        Dạng: Python string\n","        Miêu tả: Tên của activation\n","        Ví dụ: \"relu\"\n","    Đầu ra:\n","      dA_prev:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với đầu ra phép phi tuyến của lớp l-1\n","        Chiều: (Chiều của lớp phía trước, Số lượng điểm dữ liệu)\n","      dW:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với tham số của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, Chiều của lớp phía trước)\n","      db:\n","        Dạng: numpy array\n","        Miêu tả: Đạo hàm của hàm mất mát với bias của lớp hiện tại (l)\n","        Chiều: (Chiều của lớp hiện tại, 1)\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    linear_cache, activation_cache = cache\n","    \n","    # Thực hiện activation backward\n","    # Từ dA và activation_cache để tính ra dZ\n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","    \n","    # Hàm này có thể thiết kế để sử dụng nhiều hàm khác nhau trong tương lai\n","    # Bạn không cần code elif này\n","    elif activation == \"gelu\":\n","        pass\n","    \n","    # Thực hiện linear backward\n","    # Từ dZ và linear_cache để tính ra dA_prev, dW và db\n","    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"markdown","metadata":{"id":"oaOh_-h9Wxat"},"source":["Test code"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1051,"status":"ok","timestamp":1682078235006,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"5aOjEaskWzKN","outputId":"3deb0d80-feca-475b-d69a-e0078e2be909"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chiều của dA1: (256, 60000)\n","Chiều của dW: (128, 256)\n","Chiều của db: (128, 1)\n","Giá trị dA1 đã được tính đúng?: True\n","Giá trị dW đã được tính đúng?: True\n","Giá trị db đã được tính đúng?: True\n"]}],"source":["try:\n","  np.random.seed(42)\n","  dA2 = np.full((128, 60000), 4.)\n","  activation = 'relu'\n","  A1 = np.ones((256, 60000))\n","  W2 = np.full((128, 256), 2.)\n","  b2 = np.ones((128,1))\n","  linear_cache = (\n","      A1, W2, b2\n","  )\n","  activation_cache = Z = np.full((128, 60000), 2.5)\n","  cache = (linear_cache, activation_cache)\n","\n","  dA1, dW, db = linear_activation_backward(dA2, cache, activation)\n","  \n","\n","  print('Chiều của dA1: {}'.format(dA1.shape))\n","  print('Chiều của dW: {}'.format(dW.shape))\n","  print('Chiều của db: {}'.format(db.shape))\n","  print('Giá trị dA1 đã được tính đúng?: {}'.format(np.array_equal(dA1, np.full(dA1.shape, 1024.))))\n","  print('Giá trị dW đã được tính đúng?: {}'.format(np.array_equal(dW, np.full(W2.shape, 4.))))\n","  print('Giá trị db đã được tính đúng?: {}'.format(np.array_equal(E2, np.full(E2.shape, 1.))))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"CzcH7WU1Qrk6"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Chiều của dA1: (256, 60000)\n","Chiều của dW: (128, 256)\n","Chiều của db: (128, 1)\n","Giá trị dA1 đã được tính đúng?: True\n","Giá trị dW đã được tính đúng?: True\n","Giá trị db đã được tính đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"lHXPbi_juPDe"},"source":["### 6.4 - Cập nhật tham số\n","\n","Trong phần này, bạn sẽ thực hiện cập nhật các tham số:\n","\n","\n","$$ \\textbf{W}^{(l)} = \\textbf{W}^{(l)} - \\alpha \\text{ } d\\textbf{W}^{(l)} \\tag{16}$$\n","$$ \\textbf{b}^{(l)} = \\textbf{b}^{(l)} - \\alpha \\text{ } d\\textbf{b}^{(l)} \\tag{17}$$\n","\n","trong đó $\\alpha$ là learning rate. Sau khi tính cập nhật giá trị Parameters, chúng ta sẽ lưu trữ trong trong 1 parameters dictionary. "]},{"cell_type":"markdown","metadata":{"id":"RmDVbzIbuPDe"},"source":["**TODO 6**: Thực hiện `update_parameters()`.\n","\n","**Hướng dẫn**: Cập nhật tham số sử dụng Gradient Descent cho $\\textbf{W}^{(l)}$ và $\\textbf{b}^{(l)}$ của từng layer $l = 1, 2, ..., L$. \n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078235006,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"Aug2qIkVuPDe"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Lặp qua các lớp và cập nhật tham số sử dụng Gradient Descent\n","    Đầu vào:\n","      parameters:\n","        Dạng: Python dictionary \n","        Miêu tả: Chứa các tham số với key là tên và giá trị là numpy array tham số đó\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","      grads:\n","        Dạng: Python dictionary \n","        Miêu tả: Chứa gradient của các tham số với key là tên gradient và giá trị là numpy array của gradient\n","        Ví dụ: {\n","          dW1: ...,\n","          db1: ...,\n","          ...\n","        }\n","    Đầu ra:\n","      parameters:\n","        Dạng: Python dictionary \n","        Miêu tả: Chứa các tham số đã được cập nhật gradient\n","        Ví dụ: {\n","          W1: ...,\n","          b1: ...,\n","          ...\n","        }\n","    \"\"\"\n","    # Lập trình tại đây\n","\n","    # Tính số lượng lớp\n","    L = len(parameters) // 2 \n","\n","    # Lặp qua các lớp và cập nhật tham số W và b\n","    for l in range(L):\n","        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l+1)]\n","        \n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"KVt3Fqnl7UhJ"},"source":["Test code"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078235006,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"WBlqTwWD7XH5","outputId":"201931ce-91f5-488f-dd84-a33d63948ae5"},"outputs":[{"name":"stdout","output_type":"stream","text":["W1 được cập nhật đúng?: True\n","b1 được cập nhật đúng?: True\n","W2 được cập nhật đúng?: True\n","b2 được cập nhật đúng?: True\n","W3 được cập nhật đúng?: True\n","b3 được cập nhật đúng?: True\n"]}],"source":["try:\n","  n = 784\n","  d_1 = 128\n","  d_2 = 256\n","  d_3 = 1028\n","  np.random.seed(42)\n","  parameters = {\n","      \"W1\": np.random.randn(d_1, n),\n","      \"b1\": np.zeros((d_1, 1)),\n","      \"W2\": np.random.randn(d_2, d_1),\n","      \"b2\": np.zeros((d_2, 1)),\n","      \"W3\": np.random.randn(d_3, d_2),\n","      \"b3\": np.zeros((d_3, 1)),\n","  }\n","\n","  grads = {\n","      \"dW1\": np.full((d_1, n), -0.02),\n","      \"db1\": np.full((d_1, 1), 0.1),\n","      \"dW2\": np.full((d_2, d_1), -0.4),\n","      \"db2\": np.full((d_2, 1), 0.5),\n","      \"dW3\": np.full((d_3, d_2), 0.6),\n","      \"db3\": np.full((d_3, 1), -0.02),\n","  }\n","\n","  lr = 0.01\n","  updated_parameters = update_parameters(parameters, grads, lr)\n","  exp = {'W1': 106.015, 'b1': -0.128, 'W2': 200.643, 'b2': -1.28, 'W3': -1661.189, 'b3': 0.206}\n","  for key in updated_parameters:\n","    print(\"{} được cập nhật đúng?: {}\".format(key, exp[key] == np.round(np.sum(updated_parameters[key]), 3)))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"7Y-BfyHGQt2S"},"source":["**Kết quả mong đợi**: \n","\n","```\n","W1 được cập nhật đúng?: True\n","b1 được cập nhật đúng?: True\n","W2 được cập nhật đúng?: True\n","b2 được cập nhật đúng?: True\n","W3 được cập nhật đúng?: True\n","b3 được cập nhật đúng?: True\n","```"]},{"cell_type":"markdown","metadata":{"id":"anM9Ho3fMPVo"},"source":["##  7 - Train Model "]},{"cell_type":"markdown","metadata":{"id":"osy4dRRiRa1E"},"source":["### 7.1. Tải dữ liệu MNIST"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7398,"status":"ok","timestamp":1682078242402,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"iVlUEU7pceyr","outputId":"7160ab13-149e-4878-f166-3d34d4c61045"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}],"source":["import tensorflow as tf\n","(X_train, Y_train), (X_val, Y_val) = tf.keras.datasets.mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"e2mD77mIutDs"},"source":["Một số thông tin quan trọng"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682078242403,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"vzRTUc-cuu_6","outputId":"336860bc-5d22-4a2b-cf4c-1c0c3a74e884"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","  Số lượng ảnh train: 60000\n","  Chiều dài ảnh train: 28\n","  Chiều cao ảnh train: 28\n","  Chiều ảnh được duỗi: 784\n","\n"]}],"source":["# Số lượng nhãn \n","num_classes = 10\n","\n","num_of_train_images, width, height = X_train.shape\n","\n","# Chiều ảnh được duỗi\n","image_vector_size = width * height\n","\n","\n","print(\"\"\"\n","  Số lượng ảnh train: {}\n","  Chiều dài ảnh train: {}\n","  Chiều cao ảnh train: {}\n","  Chiều ảnh được duỗi: {}\n","\"\"\".format(num_of_train_images, width, height, image_vector_size))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"egzxTzYNRfEV"},"source":["### 7.2. Tiền xử lý dữ liệu"]},{"cell_type":"markdown","metadata":{"id":"Is0v-oBj0b8t"},"source":["Các hàm này được sử dụng từ bài Softmax Regression, nếu bạn chưa hiểu kỹ bài tập này thì hãy làm bài tập đó trước nhé ;)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436,"status":"ok","timestamp":1682081052243,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"AwyBNz8W1--p","outputId":"8b51fa21-0e99-4e70-dd41-4dff1ebb0dae"},"outputs":[{"data":{"text/plain":["array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["np.eye(10)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":629,"status":"ok","timestamp":1682078243030,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"FE1ZkXaoWgIl"},"outputs":[],"source":["def one_hot(y, num_classes):\n","  \"\"\"\n","  Đầu vào:\n","    y: \n","      Dạng: numpy array \n","      Miêu tả: Các giá trị nhãn\n","      Chiều: (batch_size,)\n","    num_classes: \n","      Dạng: Python integer\n","      Miêu tả: Số lượng nhãn\n","      Điều kiện: num_classes > 0\n","      Ví dụ: 10\n","  Đầu ra:\n","    y_one_hot: \n","      Miêu tả: Trả về ma trận các vector one hot của từng nhãn\n","      Chiều: (batch_size, num_classes)\n","  \"\"\"\n","  y_one_hot = np.squeeze(np.eye(num_classes)[y.reshape(-1)])\n","\n","  return y_one_hot\n","\n","def flatten_images(images):\n","  \"\"\"\n","  Thực thi duỗi ảnh từ (batch_size, width, height) thành (batch_size, width x height)\n","  Đầu vào:\n","    images: \n","      Dạng: numpy array \n","      Miêu tả: Ma trận các ảnh\n","      Chiều: (batch_size, width, height)\n","      Ví dụ: (32, 28, 28)\n","  Đầu ra:\n","    flattened_images\n","      Miêu tả: ma trận trong đó các ảnh được duỗi thành vector\n","      Chiều: (batch_size, image_vector_size) = (batch_size, width x height)\n","      Ví dụ: (32, 28 x 28) = (32, 784)\n","  \"\"\"\n","  flattened_images = np.reshape(images, (images.shape[0], -1))\n","\n","  return flattened_images\n","\n","def normalize_images(images):\n","  \"\"\"\n","  Hàm chuẩn hóa ảnh các giá trị pixel để nằm trong khoảng từ 0 đến 1\n","  Đầu vào:\n","    images: \n","      Dạng: numpy array \n","      Miêu tả: Ma trận các vector ảnh\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  Đầu ra:\n","    normailized_images\n","      Miêu tả: ma trận trong đó các vector ảnh được chuẩn hóa\n","      Chiều: (batch_size, image_vector_size)\n","      Ví dụ: (32, 784)\n","  \"\"\"\n","  normailized_images = images / 255.0\n","\n","  return normailized_images\n","\n","try:\n","  X_train = flatten_images(X_train)\n","  X_val = flatten_images(X_val)\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)\n","\n","try:\n","  X_train = normalize_images(X_train)\n","  X_val = normalize_images(X_val)\n","except Exception as e:\n","   print(\"Lỗi thực thi: \", e)\n","\n","# Chuyển nhãn thành one hot\n","# y_train = one_hot(Y_train, 10)\n"]},{"cell_type":"markdown","metadata":{"id":"AqztuH9zeTOe"},"source":["### 7.3. Định nghĩa chiều các lớp"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682078243030,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"4btCQmPABl0B"},"outputs":[],"source":["n = image_vector_size\n","d_1 = 1024\n","d_2 = 256\n","c = 10\n","\n","layers_dims = (n, d_1, d_2, c)"]},{"cell_type":"markdown","metadata":{"id":"29Y2QRPqtPVH"},"source":["### 7.4. Xây dựng mô hình"]},{"cell_type":"markdown","metadata":{"id":"jxhPHl7metoN"},"source":["**TODO 7**: Lập trình hàm tính độ chính xác"]},{"cell_type":"markdown","metadata":{"id":"2yNLprEw0qoT"},"source":["Trong trường hợp này trong thân hàm cần chú ý tới chiều\n","\n","\n","- $n: $ chiều vector ảnh (784)\n","- $m: $ số ảnh (60000)\n","- $c: $ số nhãn (10)\n","\n","Chiều của $\\textbf{X} \\in \\mathbf{R} ^ {n \\times m}$\n","Chiều của $\\textbf{Y} \\in \\mathbf{R} ^ {m \\times 1}$\n"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682081236246,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"xJHHg064vgUa"},"outputs":[],"source":["def cal_acc(parameters, X, Y):\n","  \"\"\"\n","  Tính độ chính xác của mô hình trên toàn bộ tập dữ liệu\n","    Bước 1: Thực hiện feed forward để lấy kết quả dự đoán\n","    Bước 2: So sánh kết quả dự đoán với nhãn thật\n","  Đầu vào:\n","    X: \n","      Dạng: numpy array \n","      Miêu tả: Các vector ảnh\n","      Chiều: (image_vector_size, Số lượng điểm dữ liệu)\n","      Ví dụ: (784, 60000)\n","    Y: \n","      Dạng: numpy arry\n","      Miêu tả: Vector của nhãn\n","      Chiều: (Số lượng điểm dữ liệu,)\n","      Ví dụ: (60000,)\n","  Đầu ra:\n","    acc:\n","      Dạng: số thực\n","      Miêu tả: Độ chính xác của mô hình tại thời điểm hiện tại\n","      Ví dụ: 0.6\n","  \"\"\"\n","  # Lập trình tại đây\n","\n","  W1 = parameters[\"W1\"]\n","  b1 = parameters[\"b1\"]\n","  W2 = parameters[\"W2\"]\n","  b2 = parameters[\"b2\"]\n","  W3 = parameters[\"W3\"]\n","  b3 = parameters[\"b3\"]\n","\n","  A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n","  A2, cache2 = linear_activation_forward(A1, W2, b2, 'relu')\n","  A3, cache3 = linear_activation_forward(A2, W3, b3, 'softmax')\n","\n","  # (Số lượng điểm dữ liệu, số lượng nhãn)\n","  Y_hat = A3.T\n","  acc = np.sum(np.equal(np.argmax(Y_hat, axis=1), Y).astype(float)) / Y.shape[0]\n","  \n","  return acc"]},{"cell_type":"markdown","metadata":{"id":"Rg0DUEp_OpZv"},"source":["Test code"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682081236892,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"Fhz9UoQzOsWo","outputId":"9b465b96-12b0-4eff-e7c2-4401ffa53ed3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 4) (4,)\n","(4, 10)\n","(4,)\n","---\n","Độ chính xác: 0.500\n"]}],"source":["try:\n","  np.random.seed(42)\n","  parameters = {\n","    \"W1\": np.random.randn(d_1, n),\n","    \"b1\": np.zeros((d_1, 1)),\n","    \"W2\": np.random.randn(d_2, d_1),\n","    \"b2\": np.zeros((d_2, 1)),\n","    \"W3\": np.random.randn(num_classes, d_2),\n","    \"b3\": np.zeros((num_classes, 1)),\n","  }\n","\n","  Y_mock =  np.array([5,2,6, 4])\n","  X_mock = X_mock = X_train[:4].T / 255.0\n","  acc_mock = cal_acc(parameters, X_mock, Y_mock)\n","  print(\"Độ chính xác: {:.3f}\".format(acc_mock))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"MzJuCLa9Q2-2"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Độ chính xác: 0.500\n","```"]},{"cell_type":"markdown","metadata":{"id":"NqfxKw9xOspa"},"source":["**TODO 8** Xây dựng mô hình và tiến hành training\n"]},{"cell_type":"markdown","metadata":{"id":"rEs4jQG9e0Mm"},"source":["Với 2 lớp ta sẽ có các công thức sau.\n","\n","\n","**Chiều Backward:**\n","\n","- Lớp Softmax cuối cùng: \n","\n","$$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","$$ d\\textbf{A}^{(2)} = \\textbf{W}^{(3)T}\\textbf{E}^{(3)} $$\n","\n","\n","$$ d\\textbf{W}^{(3)} = \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T} \\rightarrow \\textbf{W}^{(3)} := \\textbf{W}^{(3)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(3)}\\textbf{A}^{(2)T}    $$\n","\n","$$ d\\textbf{b}^{(3)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}\\rightarrow \\textbf{b}^{(3)} := \\textbf{b}^{(3)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(3)(i)}$$\n","\n","- Lớp ẩn số 2: \n","\n","$$\\textbf{E}^{(2)} = d\\textbf{Z}^{(2)} = {d\\textbf{A}^{(2)}} \\odot f'(\\textbf{Z}^{(2)}) $$\n","\n","$$ d\\textbf{A}^{(1)} = \\textbf{W}^{(2)T}\\textbf{E}^{(2)} $$\n","\n","$$ d\\textbf{W}^{(2)} = \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T} \\rightarrow \\textbf{W}^{(2)} := \\textbf{W}^{(2)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(2)}\\textbf{A}^{(1)T}    $$\n","\n","$$ d\\textbf{b}^{(2)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}\\rightarrow \\textbf{b}^{(2)} := \\textbf{b}^{(2)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(2)(i)}$$\n","\n","\n","- Lớp ẩn số 1: \n","\n","$$\\textbf{E}^{(1)} = d\\textbf{Z}^{(1)} = {d\\textbf{A}^{(1)}} \\odot f'(\\textbf{Z}^{(1)}) $$\n","\n","$$ d\\textbf{W}^{(1)} = \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T} \\rightarrow \\textbf{W}^{(1)} := \\textbf{W}^{(1)}  - \\alpha \\frac{1}{m} \\textbf{E}^{(1)}\\textbf{X}^{T}    $$\n","\n","$$ d\\textbf{b}^{(1)} = \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}\\rightarrow \\textbf{b}^{(1)} := \\textbf{b}^{(1)}  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\textbf{E}^{(1)(i)}$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lt1hTST0Kc-F"},"source":["Nếu bạn thắc mắc tại sao $$\\textbf{E}^{(3)} = \\hat{\\textbf{Y}} - \\textbf{Y} $$\n","\n","thì lời giải nằm [tại đây](https://colab.research.google.com/drive/1awivtxQzjgNK2m-cEQLIyzE5g6MxcZpX#scrollTo=NYrcc6YWGz1S)."]},{"cell_type":"markdown","metadata":{"id":"yi2yxBGsTdHR"},"source":["**TODO 9:** Xây dựng hàm train model"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682078243031,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"OnePW1bgNNv0"},"outputs":[],"source":["def train_model(X_train, Y_train, X_val, Y_val, layers_dims, learning_rate=0.1, epochs=250):\n","    \"\"\"\n","    Hàm xây dựng và train model\n","      Bước 1: Thực hiện lan truyền thuận, tính giá trị đầu ra của các lớp\n","      Bước 2: Tính giá trị mất mát\n","      Bước 3: Thực hiện lan truyền ngược, tính giá trị đạo hàm của hàm mất mát trên các tham số\n","      Bước 4: Cập nhật tham số của mô hình\n","    Đầu vào:\n","      X_train: \n","        Dạng: numpy array \n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (60000, 784)\n","      Y_train: \n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (60000,)\n","      X_val: \n","        Dạng: numpy array \n","        Miêu tả: Các vector ảnh\n","        Chiều: (Số lượng điểm dữ liệu, image_vector_size)\n","        Ví dụ: (10000, 784)\n","      Y_val: \n","        Dạng: numpy arry\n","        Miêu tả: Vector của nhãn\n","        Chiều: (Số lượng điểm dữ liệu,)\n","        Ví dụ: (10000,)\n","      layers_dims:\n","        Dạng: Python tuple\n","        Miêu tả: (n, d_1, d_2, num_classes)\n","          n: Chiều của vector ảnh\n","          d_1: Chiều của lớp ẩn 1\n","          d_2: Chiều của lớp ẩn 2\n","          num_classes: số nhãn\n","      learning_rate:\n","        Dạng: Python float\n","        Miêu tả: Tốc độ học của mô hình\n","        Ví dụ: 0.001\n","      epochs:\n","        Dạng: Python integer\n","        Miêu tả: Số vòng lặp qua tập dữ liệu\n","        Ví dụ: 100\n","    Đầu ra:\n","      Python Tuple: (parameters, acc, val_acc, costs)\n","      parameters:\n","        Dạng: Python Dictionary\n","        Miêu tả: Bộ tham số của mô hình\n","      acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ train\n","        Ví dụ: 0.6\n","      val_acc:\n","        Dạng: số thực\n","        Miêu tả: Độ chính xác của mô hình trên bộ validation\n","        Ví dụ: 0.7\n","      costs:\n","        Dạng: Python List\n","        Miêu tả: Giá trị mất mát trên từng epoch\n","        Ví dụ [0.2, 0.15, 0.1]\n","    \"\"\"\n","\n","    np.random.seed(42)\n","    grads = {}\n","    costs = []                              \n","\n","    m = X_train.shape[0]\n","\n","    (n, d_1, d_2, num_classes) = layers_dims\n","    \n","    # 0. Khởi tạo tham số\n","    parameters = initialize_parameters(n, d_1, d_2, num_classes)\n","\n","    # 1. Điều chỉnh chiều dữ liệu sao cho\n","    # X_train: (image_vector_size, Số lượng điểm dữ liệu)\n","    # Y_train_one_hot: (Số lượng nhãn, Số lượng điểm dữ liệu)\n","    # X_val: (image_vector_size, Số lượng điểm dữ liệu)\n","\n","    X_train = X_train.T\n","    Y_train_one_hot = one_hot(Y_train, num_classes).T\n","    X_val = X_val.T\n","    \n","\n","    # 2. Lấy các tham số ra khỏi dictionary: parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    W3 = parameters[\"W3\"]\n","    b3 = parameters[\"b3\"]\n","\n","    # 3. Lấy các tham số ra khỏi dictionary: parameters\n","    for i in range(0, epochs):\n","\n","        # 3.1. Thực hiện Feed Forward trên 3 lớp: 2 lớp relu và lớp Softmax\n","        A1, cache1 = linear_activation_forward(X_train, W1, b1, 'relu')\n","        A2, cache2 = linear_activation_forward(A1, W2, b2, 'relu')\n","        A3, cache3 = linear_activation_forward(A2, W3, b3, 'softmax')\n","\n","        Y_hat = A3\n","\n","        # 3.2. Tính giá trị mất mát\n","        cost = compute_cost(Y_hat, Y_train_one_hot)\n","        \n","        # 3.3. Tính giá trị E3\n","        E3 = Y_hat - Y_train_one_hot\n","\n","        # 3.4. Tính giá trị dA2\n","        dA2 = W3.T.dot(E3)\n","\n","        # 3.5. Tính gía trị dW3\n","        dW3 = 1/m * np.dot(E3, A2.T)\n","\n","        # 3.6. Tính giá trị db3 \n","        # db3 = 1/m * np.sum(E3) # <---- Bug \n","        # Fix: \n","        db3 = 1/m * np.sum(E3, keepdims=True, axis=1)\n","\n","        # Tính Accuracy\n","\n","        # 3.7. Tính độ chính xác trên tập train\n","        acc = cal_acc(parameters, X_train, Y_train)\n","\n","        # 3.8. Tính độ chính xác trên tập validation\n","        val_acc = cal_acc(parameters, X_val, Y_val)\n","        \n","        # 3.9. Thực hiện backward trên lớp số 2 để tính ra dA1, dW2, db2\n","        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'relu')\n","\n","        # 3.10. Thực hiện backward trên lớp số 1 để tính ra dA0, dW1, db1\n","        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n","        \n","        # 3.11. Cập nhật các giá trị dW1, db1, dW2, db2, dW3, db3 vào dictionary grads\n","        grads['dW1'] = dW1\n","        grads['db1'] = db1\n","        grads['dW2'] = dW2\n","        grads['db2'] = db2\n","        grads['dW3'] = dW3\n","        grads['db3'] = db3\n","        \n","        # 3.12. Tiến hành cập nhật các tham số của mô hình\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        # 3.13. Gán các giá trị tham số mới vào các biến đã được định nghĩa sẵn\n","        W1 = parameters[\"W1\"]\n","        b1 = parameters[\"b1\"]\n","        W2 = parameters[\"W2\"]\n","        b2 = parameters[\"b2\"]\n","        W3 = parameters[\"W3\"]\n","        b3 = parameters[\"b3\"]\n","        \n","        print(\"Epoch {}: Cost: {:.3f} Acc: {:.3f} Validation Acc: {:.3f}\".format(i + 1, np.squeeze(cost), acc, val_acc))\n","        costs.append(cost)\n","       \n","    \n","    return parameters, acc, val_acc, costs"]},{"cell_type":"markdown","metadata":{"id":"T_Vg6Q33R3d2"},"source":["Test code"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682078243032,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"J7i-hWNQkZmu","outputId":"7d9aff75-58a3-4f05-9430-da52cad77dd8"},"outputs":[{"data":{"text/plain":["((60000, 784), (60000,), (10000, 784), (10000,))"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, Y_train.shape, X_val.shape, Y_val.shape"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39793,"status":"ok","timestamp":1682078282820,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"7UwWV5ltR2wk","outputId":"b596917f-428c-4ce9-9f00-c9cb60332e7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","W1 được cập nhật chưa đúng.\n","b1 được cập nhật chưa đúng.\n","W2 được cập nhật chưa đúng.\n","b2 được cập nhật chưa đúng.\n","W3 được cập nhật đúng.\n","b3 được cập nhật đúng.\n"]}],"source":["try:\n","  parameters, acc, val_acc, costs  = train_model(X_train, Y_train, X_val, Y_val, layers_dims = layers_dims, epochs=2)\n","  exp = {'W1': -3.096, 'W2': -10.642, 'W3': -0.124, 'b1': 0.002, 'b2': 0.007, 'b3': 0.0 }\n","  for param in parameters:\n","    if exp[param] == np.round(np.sum(parameters[param]), 3):\n","      print(\"{} được cập nhật đúng.\".format(param))\n","    else:\n","      print(\"{} được cập nhật chưa đúng.\".format(param))\n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"a3COa7ZcSJ_5"},"source":["**Kết quả mong đợi**: \n","\n","```\n","Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","W1 được cập nhật đúng.\n","b1 được cập nhật đúng.\n","W2 được cập nhật đúng.\n","b2 được cập nhật đúng.\n","W3 được cập nhật đúng.\n","b3 được cập nhật đúng.\n","```"]},{"cell_type":"markdown","metadata":{"id":"ACVkTBjjTkLH"},"source":["### 7.5. Tiến hành training"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"elapsed":89489,"status":"error","timestamp":1682078372288,"user":{"displayName":"Ngọc Nguyễn","userId":"00234888355303416315"},"user_tz":-420},"id":"HE_sYT2bSYFW","outputId":"11942b52-cdb3-4f8e-9b75-6616710be079"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: Cost: 2.303 Acc: 0.086 Validation Acc: 0.081\n","Epoch 2: Cost: 2.302 Acc: 0.092 Validation Acc: 0.085\n","Epoch 3: Cost: 2.302 Acc: 0.123 Validation Acc: 0.117\n","Epoch 4: Cost: 2.302 Acc: 0.178 Validation Acc: 0.174\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-09826ada9705>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-ea8482887623>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, Y_train, X_val, Y_val, layers_dims, learning_rate, epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# 3.7. Tính độ chính xác trên tập train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# 3.8. Tính độ chính xác trên tập validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-314dd0db828d>\u001b[0m in \u001b[0;36mcal_acc\u001b[0;34m(parameters, X, Y)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mA3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d865ccb341da>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Ta sẽ phân loại việc biến đổi tuyến tính tùy theo đầu vào activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-5aeae6401eab>\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A_pre, W, b)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_pre\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["try:\n","  # plot the cost\n","  epochs = 400\n","  learning_rate = 0.1\n","  parameters, acc, val_acc, costs  = train_model(X_train, Y_train, X_val, Y_val, layers_dims = layers_dims, learning_rate=learning_rate, epochs=epochs)\n","  \n","except Exception as e:\n","  print(\"Lỗi thực thi: \", e)"]},{"cell_type":"markdown","metadata":{"id":"0KNolg09TuSI"},"source":["**Kết quả mong đợi (Tương đối)**: \n","\n","```\n","Epoch 130: Cost: 0.416 Acc: 0.877 Validation Acc: 0.884\n","Epoch 131: Cost: 0.412 Acc: 0.883 Validation Acc: 0.886\n","Epoch 132: Cost: 0.410 Acc: 0.879 Validation Acc: 0.885\n","Epoch 133: Cost: 0.407 Acc: 0.885 Validation Acc: 0.887\n","Epoch 134: Cost: 0.406 Acc: 0.880 Validation Acc: 0.887\n","Epoch 135: Cost: 0.403 Acc: 0.886 Validation Acc: 0.888\n","Epoch 136: Cost: 0.403 Acc: 0.881 Validation Acc: 0.889\n","Epoch 137: Cost: 0.400 Acc: 0.886 Validation Acc: 0.888\n","Epoch 138: Cost: 0.401 Acc: 0.882 Validation Acc: 0.890\n","Epoch 139: Cost: 0.398 Acc: 0.887 Validation Acc: 0.888\n","Epoch 140: Cost: 0.400 Acc: 0.882 Validation Acc: 0.890\n","Epoch 141: Cost: 0.397 Acc: 0.887 Validation Acc: 0.888\n","Epoch 142: Cost: 0.399 Acc: 0.882 Validation Acc: 0.889\n","Epoch 143: Cost: 0.396 Acc: 0.887 Validation Acc: 0.888\n","Epoch 144: Cost: 0.399 Acc: 0.881 Validation Acc: 0.890\n","Epoch 145: Cost: 0.395 Acc: 0.887 Validation Acc: 0.888\n","Epoch 146: Cost: 0.398 Acc: 0.881 Validation Acc: 0.890\n","Epoch 147: Cost: 0.394 Acc: 0.886 Validation Acc: 0.888\n","Epoch 148: Cost: 0.396 Acc: 0.881 Validation Acc: 0.890\n","Epoch 149: Cost: 0.392 Acc: 0.886 Validation Acc: 0.888\n","Epoch 150: Cost: 0.394 Acc: 0.882 Validation Acc: 0.891\n","Epoch 151: Cost: 0.389 Acc: 0.887 Validation Acc: 0.889\n","Epoch 152: Cost: 0.389 Acc: 0.884 Validation Acc: 0.892\n","Epoch 153: Cost: 0.384 Acc: 0.889 Validation Acc: 0.890\n","Epoch 154: Cost: 0.383 Acc: 0.887 Validation Acc: 0.893\n","Epoch 155: Cost: 0.378 Acc: 0.891 Validation Acc: 0.892\n","Epoch 156: Cost: 0.376 Acc: 0.890 Validation Acc: 0.896\n","Epoch 157: Cost: 0.372 Acc: 0.893 Validation Acc: 0.895\n","Epoch 158: Cost: 0.370 Acc: 0.893 Validation Acc: 0.899\n","Epoch 159: Cost: 0.367 Acc: 0.895 Validation Acc: 0.898\n","Epoch 160: Cost: 0.364 Acc: 0.895 Validation Acc: 0.901\n","Epoch 161: Cost: 0.361 Acc: 0.897 Validation Acc: 0.899\n","Epoch 162: Cost: 0.359 Acc: 0.897 Validation Acc: 0.902\n","Epoch 163: Cost: 0.357 Acc: 0.899 Validation Acc: 0.901\n","Epoch 164: Cost: 0.355 Acc: 0.899 Validation Acc: 0.904\n","Epoch 165: Cost: 0.353 Acc: 0.900 Validation Acc: 0.902\n","Epoch 166: Cost: 0.352 Acc: 0.900 Validation Acc: 0.904\n","Epoch 167: Cost: 0.350 Acc: 0.901 Validation Acc: 0.903\n","Epoch 168: Cost: 0.349 Acc: 0.901 Validation Acc: 0.906\n","Epoch 169: Cost: 0.347 Acc: 0.902 Validation Acc: 0.904\n","Epoch 170: Cost: 0.346 Acc: 0.902 Validation Acc: 0.906\n","```"]},{"cell_type":"markdown","metadata":{"id":"4WyVklEEKNFp"},"source":["Chúc mừng bạn đã vượt qua một thử thách lớn trong con đường học AI của mình."]},{"cell_type":"markdown","metadata":{"id":"8Fux1mJWKeqU"},"source":["<img src=\"https://storage.googleapis.com/protonx-cloud-storage/icons/488-bicycle-outline.gif\" />"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1dQtbWnc0r1LoKMQLZYNRhwPX6VXCWwll","timestamp":1603544333221},{"file_id":"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb","timestamp":1597203092580}]},"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
